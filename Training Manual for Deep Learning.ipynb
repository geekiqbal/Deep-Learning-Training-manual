{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Guiding Principles\n",
    "\n",
    "Training a deep learning model is an iterative process of, as I put it, systematic trial and error. You have to do a lot of experimentation, try different hyperparameters and repeat experiments to find out the best solution.\n",
    "\n",
    "Sometimes, the problem is simple; you have ample data and computational resources and time. Other times (most of the times actually), you run short on either (or all) of these things. With so many things to tune, it's very easy to get distracted and waste a lot of time on the wrong things. No matter what problem I'm given, there are a set of guiding principles that I've learned from mentors or by experimenting on my own which make sure that I'm always on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tune one hyperparameter at a time\n",
    "\n",
    "Always do an iteration with a single hyperparameter update. Don't tune two hyperparameters at once. Ever. You'll never figure out which hyperparameter update improved performance. This is the golden rule, **one update at a time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Always start with optimal hyperparameter values from literature\n",
    "\n",
    "Instead of choosing hyperparameter values randomly, it's best to start with the values tried and tested by other people. You'll probably end up changing the value but if it has worked for someone else, it might work for you as well. For example, the value used for learning rate (alpha) in most papers is 0.0001. The most common stride value used is 1 for CNNs. Relu is the go to non-linear function. Always start from the most common values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Intuition doesn't transfer across domains\n",
    "\n",
    "If you've worked on a problem in, say, text classification with CNNs, the hyperparameter values won't transfer to, say, a cancer classification CNN. Every industry is different and every problem is different and your appraoch should be to start with a fresh mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Experiment Faster with samples\n",
    "\n",
    "Instead of using the whole dataset for experimentation, use the following recipe;\n",
    "\n",
    "1. Overfit on 100-500 examples, first thing\n",
    "2. Once your model achieves 100% training accuracy on task 1, train on 10% of the dataset.\n",
    "3. If it gets 100% accuracy on 10% dataset, run on all of the dataset.\n",
    "\n",
    "If you had to, say, make a 100 changes in the initial model, this recipe ensures that your 95% changes are made within first 2 phases of the recipe with a much smaller dataset, which saves time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Always ask yourself if it can be solved with a simpler model\n",
    "\n",
    "Deep Learning is the cool new sh\\*t and all BUT always, always (always) ask yourself; do you really need deep learning to solve a problem? What benefit would you get if you used deep learning instead of some other technique? Would you save time? It's very hard to fix deep learning models if they get stuck in an error (you'd need a lot of background in maths) and it's even harder to interpret what's going on under the hood. Unless you have a very strong reason to not use a simpler model, avoid using a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Designing a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 V.V.Important: No Bells & Whistles\n",
    "\n",
    "Always always start with a simple model. People usualy throw in everything they find in a deep learning library the first time they're training a model. Never Ever Do That. Follow the follwoing simple instructions;\n",
    "\n",
    "1. Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.\n",
    "2. Drop everything e.g. regularization and data augmentation and batch norm.\n",
    "3. If finetuning a model, **double check the preprocessing, for it should be the same as the original modelâ€™s training.**\n",
    "4. Verify that the input data is correct.\n",
    "5. Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Choose Number of Layers\n",
    "\n",
    "- Start with a small number of layers and try to overfit your data.\n",
    "- Keep increasing number of epochs while keeping everything else standard as long as training accuracy is increasing\n",
    "- When training accuracy stops increasing despite increasing number of epochs, increase number of layers. Your model is not complex enough to learn variance in your data.\n",
    "- Once you've hit 100% on training set, you have a complex enough network. Now improve other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choose neurons in each layer\n",
    "\n",
    "- For the first layer, depending on the number of inputs and complexity of problem, choose a number. For example for a simple problem and 20 inputs, 10-15 neurons in first layer would suffice. Whereas if you have 1000,000 inputs and a complex problem, try a 1000 neurons in the first layer.\n",
    "\n",
    "- It's usually better to increase number of layers instead of increasing number of neurons in case you want more complexity in your model but try doing the opposite as well i.e. experiemnt increasing the number of neurons in existing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build your first system quickly, then iterate\n",
    "\n",
    "- Set up train and test sets\n",
    "- Choose optimizing metric (accuracy for example)\n",
    "- Train a model\n",
    "- Fix problems by following instructions in 2.4\n",
    "- Do error analysis (details following) to figure out next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Instructions\n",
    "\n",
    "Once you have the initial model trained, follow these instructions.\n",
    "\n",
    "- Calculate accuracy on both train and test sets\n",
    "- If accuracy on both is low, you're underfitting, do the following in order\n",
    "    - Increase epochs / train for longer\n",
    "    - Increase complexity (bigger model, more layers, more neurons)\n",
    "    - Change architecture / change hyperparameters\n",
    "- If accuracy on train is better than test, do the following (once you've acheived 100% on train)\n",
    "    - Get more data\n",
    "    - Regularization\n",
    "    - Change architecture / change hyperparameters\n",
    "\n",
    "As you might have noted, changing hyperparameters is not the first thing you should do. Stick to the most common values and you'll do fine in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hyperparameters (& problem areas realted to a specific hyperparameter)\n",
    "\n",
    "- Learning Rate (convergence problems, loss not decreasing)\n",
    "- Batch Size (bad overfitting, catastrophic forgetting)\n",
    "- Number of epochs (overfitting & underfitting)\n",
    "- Regularization (overfitting)\n",
    "- Optimization Algorithms (Speed & convergence)\n",
    "- Weight Initialization (loss not decreasing)\n",
    "- Loss Function (stagnant loss from start)\n",
    "- Network Topology (underfitting)\n",
    "- Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Error Analysis\n",
    "\n",
    "Error analysis helps figure out the problems in data which are affecting your performance. By doing error analysis, you can find a reason of why your network is not performing.\n",
    "\n",
    "- Pick out samples cases from test set which are misclassified by the network\n",
    "- Study these cases for problems. Is the problem in a specific class? Is the data mislabelled (a very common problem)? Once you isolate the problem area in the dataset, figuring out a solution is a walk in the park."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Common mistakes\n",
    "\n",
    "- Not checking input; print everything to make sure it is the same thing you intend it to be.\n",
    "- Not shuffling data (bad overfitting)\n",
    "- Not checking class imbalance\n",
    "- Using accuracy as optimizing metric for imbalanced data\n",
    "- Very small batch size\n",
    "- Very large batch size\n",
    "- Not standardizing the data\n",
    "- Not initializing weights properly\n",
    "- Stacking layers without really thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Final 2 cents\n",
    "\n",
    "Try to solve a simpler version of the problem. For instance, if doing multiclass classification, use a standard dataset e.g. MNIST to diagnose problems listed above in your network. Once it's working on MNIST, switch to your own dataset. When solving multiple problems e.g. classification & localization in the same network, check each part individually to find problems. Lastly, have patience.\n",
    "\n",
    "## Good Luck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
